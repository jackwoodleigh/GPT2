### GPT2
- GPT2 (124M) implementation.
- 1024 token context length.
- Tested by overfitting the tiny shakespeare dataset. (samples in samples folder)
- Weights: https://drive.google.com/file/d/1AYRaqq-RbCCvh9DdMJWRGyS3MeClopi0/view?usp=sharing


#### Resources 
- https://github.com/karpathy/build-nanogpt
